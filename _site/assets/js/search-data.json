{
  
  "0": {
    "title": "1. Environment Configuration",
    "content": "Machine info setup - home . Code executed on ubuntu machine with nVidia GTX1080-Ti for graphics-accelerated machine learning. . To set up environment with GPU support (required to run python scripts), need to call the following commands in order in bash terminal. . 2 commands below from instructions at https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc . Install Conda to enable graphics card support . sudo apt-get install conda . Install GPU support . conda create --name tf_gpu tensorflow-gpu . Check that python version is as required (3.7 or greater) . python -v . Output on this machine satisfies this requirement: . Python 3.7.4 (default, Aug 13 2019, 20:35:49) . Install pip package manager for python . sudo apt install python3-venv python3-pip . Install requirements for python . pip3 install -r ./requirements.txt .",
    "url": "http://localhost:4000/docs/circuitSNP/code/1_preliminary_setup/",
    "relUrl": "/docs/circuitSNP/code/1_preliminary_setup/"
  }
  ,"1": {
    "title": "2. Download data",
    "content": "In this step, the goal is to obtain the data for: . 1. SNP locations http://genome.grid.wayne.edu/centisnps/compendium/ . 2. Motif locations http://genome.grid.wayne.edu/centisnps/combo/ . 3. Open chromatin in ALL and LCL regions http://genome.grid.wayne.edu/centisnps/test/ . 1. SNP locations http://genome.grid.wayne.edu/centisnps/compendium/ . Source Code: ./compendium/download_compendium.py . Execute following in bash terminal . python ./compendium/download_compendium.py . The python commands in this script are explained as follows: . List of files to download stored in file_names_combo.txt . f=open(&#39;file_names_combo.txt&#39;,&#39;r+&#39;) file_lines=f.readlines() . Files take a long time to download, so if we don’t have enough time to download all at once, save downloaded files . sf_fn=&#39;saved_files.txt&#39; saved_files=open(sf_fn,&#39;r+&#39;) . Compare both lists of files and obtain the remaining files to download . remaining_to_download=[f for f in to_download if f not in saved_fn] . Now run thru the list of files in remaining_to_download using wget: . for dl in remaining_to_download: print(home_directory_url+ dl) file_url=home_directory_url + dl wget.download(file_url, dl) saved_files.write(dl+&#39; n&#39;) num_already_dl+=1 print(&#39; n&#39; + str(num_already_dl) + &#39; successfully downloaded of &#39; + str(len(to_download))) . Once we have all files, compile by calling the following script: . Source Code: ./compendium/compendium_data_compile.py . python ./compendium/compendium_data_compile.py . 2. Motif locations http://genome.grid.wayne.edu/centisnps/combo/ . Combo reads are all pre-compiled into one file on the server: . http://genome.grid.wayne.edu/centisnps/combo/footprints.all . This file is 2.6gb and contains more records than all combo files downloaded together. . Reconciliation of files: … . Source Code: ./COMBO/download_combo.py . Execute following in bash terminal . python ./compendium/download_combo.py . This script works in a manner identical to compendium (explained above) . Once we have all files, compile by calling the following script: . Source Code: ./COMBO/combo_data_compile.py . python ./compendium/compendium_data_compile.py . 3. Open chromatin in ALL and LCL regions http://genome.grid.wayne.edu/centisnps/test/ . Source Code: ./rawdata/compendium/download_compendium.py . Execute following in bash terminal . python ./rawdata/download_bulk_data.py . This retrieves following files: . #ALL reads ALL_reads_fn=&#39;wgEncodeAwgDnaseUwdukeGm12878UniPk.narrowPeak&#39; wget.download(home_directory_url + ALL_reads_fn, ALL_reads_fn) . wget.download(home_directory_url + LCL_reads_fn, LCL_reads_fn) LCL_reads_fn=&#39;wgEncodeRegDnaseClusteredV3.bed&#39; .",
    "url": "http://localhost:4000/docs/circuitSNP/code/2_download_data/",
    "relUrl": "/docs/circuitSNP/code/2_download_data/"
  }
  ,"2": {
    "title": "3. Detect open / closed regions",
    "content": "Back to contents . The previous step retrieved the locations of open chromatin windows in LCL and ALL tissues. . The goal of this step is to use these locations to derive regions of open and closed chromatin between both tissue types. . Specifically, we want to see whether chromatin that was previously open in ALL tissue has been closed in LCL tissue, or has been kept open. . Open and closed regions of chromatin are classified as follows: . Chromatin open: footprints in region of ALL tissues overlap with footprints in region of LCL tissues . | Chromatin closed: footprints in region of ALL tissues do not overlap with any footprint in LCL tissues . | . Source Code: ./analyse_footprint_overlap.py . The python script to analyse chromatin openness is run in bash terminal: . python ./analyse_footprint_overlap.py . This project uses pandas dataframes - this data structure is helpful for manipulating tables and extracting values (this data format was copied from R). Documentation on pandas dataframe here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html . We read csv files as dataframe objects with the following call: . import pandas as pd &lt;dataframe_object&gt; = pd.read_csv(&#39;file.csv&#39;) . So now let’s load reads for ALL tissus and LCL tissues . Load ALL reads: . #ALL reads openALL = open(&quot;rawdata/wgEncodeRegDnaseClusteredV3.bed&quot;,&#39;r&#39;) #read in files as data frame usign pandas all_reads = pd.read_csv(openALL, sep=&quot; t&quot;) . Rename columns . all_reads.colnames=[&#39;chrom&#39;,&#39;start&#39;,&#39;stop&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;] all_reads.columns = all_reads.columns.str.strip() . Do same for LCL . #LCL reads openLCL = open(&quot;rawdata/wgEncodeAwgDnaseUwdukeGm12878UniPk.narrowPeak&quot;,&#39;r&#39;) #read in files as data frame usign pandas lcl_reads = pd.read_csv(openLCL, sep=&quot; t&quot;) . Create column to store whether chromatin is open or closed . #chromatin_open column refers to whether chromatin is open or closed all_reads[&quot;chromatin_open&quot;]=&quot;&quot; all_reads[&quot;chromatin_overlap&quot;]=&quot;&quot; . Comparing elements is very slow unless we use the GPU. . GPU functions can only work on arrays - they cannot work on dataframes directly. . So now extract relevant variables as arrays to be passed in. . Relevant variables are the start and stop locations of reads for ALL and LCL tissue types . #extract arrays to pass into function all_start=np.array(all_reads[&#39;start&#39;],dtype=np.int32) all_stop=np.array(all_reads[&#39;stop&#39;],dtype=np.int32) lcl_start=np.array(lcl_reads[&#39;start&#39;],dtype=np.int32) lcl_stop=np.array(lcl_reads[&#39;stop&#39;],dtype=np.int32) . The chromosome identifier is given as a string, but we need to to convert this to an int, becuase GPU cannot work on string arrays. . So create a dictionary for unique chromosome names called ‘chrom_dict’ . #get unique values for chromosome column in ALL and LCL data all_chrom=set(all_reads[&#39;chrom&#39;]) lcl_chrom=set(lcl_reads[&#39;chrom&#39;]) #Create the union of all chromosomes in either ALL and LCL data total_chrom=all_chrom.union(lcl_chrom) #as we cannot pass string values into GPU parallel function, #construct dictionary so that we can group by chromosome indexed by integer chrom_dict={} for i,chrom in enumerate(total_chrom): chrom_dict[chrom]=i . Now map chromosome dictionary values back to each read via a new column called ‘chrom_dict’ . #now use this to replace in lcl and all (convert to integer) all_reads[&#39;chrom_dict&#39;] = all_reads[&#39;chrom&#39;].map(chrom_dict) lcl_reads[&#39;chrom_dict&#39;] = lcl_reads[&#39;chrom&#39;].map(chrom_dict) . And then extract chromosome arrays for ALL and LCL tissues so that we can use them to analyse overlapping regions . #extract chromosomes as list to pass into function all_chrom=np.array(all_reads[&#39;chrom_dict&#39;],dtype=np.int32) lcl_chrom=np.array(lcl_reads[&#39;chrom_dict&#39;],dtype=np.int32) . Now setup input arrays on GPU cluster . # move input data to the GPU cluster d_all_start = cuda.to_device(all_start) d_all_stop = cuda.to_device(all_stop) d_lcl_start = cuda.to_device(lcl_start) d_lcl_stop = cuda.to_device(lcl_stop) d_all_chrom = cuda.to_device(all_chrom) d_lcl_chrom = cuda.to_device(lcl_chrom) . Set up output arrays on GPU cluster . # create arrays on GPU cluster to allow data to be retrieved from cluster d_chromatin_open = cuda.device_array_like(d_all_start) d_window_start = cuda.device_array_like(d_all_start) d_window_end = cuda.device_array_like(d_all_start) d_window_start_id = cuda.device_array_like(d_lcl_id) d_window_end_id = cuda.device_array_like(d_lcl_id) . We also have to set window length. Here have used 300. The script will raise exception if length not divisible by 2. . #set window length - must be divisible by 2 WINDOW_LENGTH=300 . Now, ready to set up function check_chromatin_open() . check_chromatin_open() performs analysis of overlapping regions, explained as follows . Define @cuda.jit to make run on GPU . #Define function check_chromatin_open to perform analysis of overlap @cuda.jit def check_chromatin_open( al_start, al_end, lc_start, lc_end, al_chrom, lc_chrom, chromatin_open, window_start, window_end, window_start_id, window_end_id): . Loop thru every read for ALL and every read for LCL. If we get any overlapping regions then make a note of this in the chromatin open state for the corresponding row in ALL. . Reads are are overlapping if the open chromatin regions overlap like this: . Read 1: #-# Read 2: #-# . # refers to a start or end point refers to a set of alleles . This depicts that read 1 terminates before read 2. Consider that read 1 might terminate after read 2: . Read 1: #--# Read 2: #-# . For this analysis, ignore where the Read 1 ends. Denote this as /: . Read 1: #-/ Read 2: #-# . Which means that read 1 might terminate before read 2 terminates, or after. It doesn’t matter - we’re only looking at which read starts the overlapping region. . Now, check for overlap with either LCL starting region, or ALL starting region. . We have two conditions to match: . LCL starts the overlapping region . First, check chromosome is the same: al_chrom[i1]==lc_chrom[i2] . Then, check that LCL starts the region . if al_chrom[i1]==lc_chrom[i2] and lc_start[i2]&lt;=al_start[i1] and al_start[i1]&lt;=lc_end[i2]: #this is case where we have: # ALL #-/ # LCL #-# chromatin_open[i1] = 1 window_start[i1]=lc_start[i2] window_start_id[i1]=i2+0.0003 . ALL starts read . First, check chromosome is the same: al_chrom[i1]==lc_chrom[i2] . Then, check that ALL starts the region . elif al_chrom[i1]==lc_chrom[i2] and al_start[i1]&lt;=lc_start[i2] and lc_start[i2]&lt;=al_end[i1]: #this is case where we have: # ALL #-/ # LCL #-# . Now, we have to make window 300bp long . #now, extend window to be 300bp as defined in WINDOW_LENGTH #borrowed from Denny Shin&#39;s code line 135 of dataprep.py half_window_length=WINDOW_LENGTH/2 #need to have window for either side centroid=window_start[i1]+window_end[i1] resized_window_start[i1]=centroid-half_window_length-1 resized_window_end[i1]=centroid+half_window_length . Finally check that the new window encapsulates limits of old window . if resized_window_start[i1]&lt;=window_start[i1] and resized_window_end[i1]&gt;=window_start[i1]: #so this corresponds to: #RESIZED WINDOW ## #WINDOW ## window_encapsulates_limits[i1]=1 #if the window steps outside the original window, flag this - might come in handy later for troubleshooting else: window_encapsulates_limits[i1]=0 . In order to call function on GPU, we must specify number of blocks = 1000 and block size = 256 . These values are chosen by trying out different values - they run ~ 6 seconds. . We must call function and then call cuda.synchronise() to wait for all cores to finish before proceeding . check_chromatin_open[1000,256]( d_all_start, d_all_stop, d_lcl_start, d_lcl_stop , d_all_chrom, d_lcl_chrom, d_chromatin_open, d_window_start, d_window_end, d_window_start_id, d_window_end_id) cuda.synchronize() . Now we can extract data from cluster using copy_to_host() syntax . #now add results to all_reads and then save out... all_reads[&#39;d_chromatin_open&#39;]=d_chromatin_open.copy_to_host() all_reads[&#39;d_window_start&#39;]=d_window_start.copy_to_host() all_reads[&#39;d_window_end&#39;]=d_window_end.copy_to_host() all_reads[&#39;d_window_start_id&#39;]=d_window_start_id.copy_to_host() all_reads[&#39;d_window_end_id&#39;]=d_window_end_id.copy_to_host() . Save results . #now write out chromatin_open... all_reads.to_csv(&#39;chromatin_reads.csv&#39;,index=False) . Back to contents .",
    "url": "http://localhost:4000/docs/circuitSNP/code/3_analyse_lcl_chromatin/",
    "relUrl": "/docs/circuitSNP/code/3_analyse_lcl_chromatin/"
  }
  ,"4": {
    "title": "4. Motif membership",
    "content": "Back to contents . In the previous step, reads were analysed to determine whether chromatin is open or closed in LCL reads, with a window size of 300bp. . The goal of the current step is to determine which motifs exist in these windows. . We use the result of chromatin open or closed as the data for current step . Source Code: ./analyse_motif_presence_in_windows.py . We call the python script in bash terminal: . python ./analyse_motif_presence_in_windows.py . Read in files . #this is our open / closed chromatin windows = pd.read_csv(&#39;chromatin_reads.csv&#39;) #already formatted as data frame #read in all motifs.. motif_reads=pd.read_csv(&#39;COMBO/all_combo_reads.csv&#39;) motif_reads.columns = [&#39;chrom&#39;,&#39;start&#39;,&#39;stop&#39;,&#39;motif&#39;,&#39;float&#39;,&#39;sign&#39;,&#39;d_motif_membership&#39;] . Very similar to comparing LCL and ALL tissue types, we use a function to compare: . the start and end positions for motif | the start and end positions for the window | . So we loop through the table of all window reads (generated in the previous step), and if the motif exists in a window, we record the index of the window (which is its row number in the table of all window reads) . function defined here . @cuda.jit def check_motif_membership( . first case - motif start precedes window . if window_chrom[i2]==motif_chrom[i1] and window_start[i2]&lt;=motif_start[i1] and motif_start[i1]&lt;=window_end[i2]: #this is case where we have: # WINDOW #-/ # MOTIF #-# motif_membership[i1]=i2 . second case - window start precedes motif . elif window_chrom[i2]==motif_chrom[i1] and motif_start[i1]&lt;=window_start[i2] and window_start[i2] &lt;= motif_end[i1]: #this is case where we have: # WINDOW #-/ # MOTIF #-# motif_membership[i1]=i2 . settings for cuda block size are [1000,256] . check_motif_membership[1000,256]( d_motif_start, d_motif_end, d_motif_chrom, d_window_start, d_window_end, d_window_chrom, d_motif_membership) . might need to fiddle with block size settings for cuda, time taken is very long . completed in: 438.7458071708679 seconds. . Append the column designating motif membership, and write out file: . motif_reads[&#39;d_motif_membership&#39;]=d_motif_membership.copy_to_host() #write out motif_reads.to_csv(&#39;motif_memberships.csv&#39;,index=False) . Back to contents .",
    "url": "http://localhost:4000/docs/circuitSNP/code/4_analyse_motif_window_membership/",
    "relUrl": "/docs/circuitSNP/code/4_analyse_motif_window_membership/"
  }
  ,"5": {
    "title": "5. Generate T matrix",
    "content": "Back to contents . In the previous step, we analysed which motifs existed in which windows. . so now, we have open / closed regions and motif membership / absence in each window. This is the data used for current step. . In the current step, we wish to generate T matrix which denotes which motifs exist or do not exist at any window, and whether that window has open or closed chromatin. . Source Code: ./generate_T_matrix.py . python ./generate_T_matrix.py . Read results from previous script to analyse chromatin open or closed . windows = pd.read_csv(&#39;chromatin_reads.csv&#39;) #already formatted as data frame . Set how many open / closed regions we want by changing these two variables . TOTAL_OPEN_CHROMATIN=100 TOTAL_CLOSED_CHROMATIN=100 . This makes no difference, since we want to include all windows. . If we want to include all windows, we have to set SUBSET_OPEN_CLOSED=0. Otherwise set to 1 if we want to subset. . So set to 0 . SUBSET_OPEN_CLOSED=0 #if this set to 0, we include all windows . Number of motif given to us by centiSNP study (Moyerbrailean et al., 2016) . NUMBER_OF_MOTIFS=1372 . If we are not using all windows, then we have to sample from open and closed windows separately. . open regions subsampling . note we have to use ‘sample’ from random.sample to generate permuted sample (randomly selected from all open chromatin regions) . #open open_window_indices=open_windows[&#39;window_index&#39;] #use &#39;sample&#39; to generate permutation open_window_sample_indices=sample(set(open_window_indices),100) samp_chromatin_open=open_windows[open_windows.window_index.isin(open_window_sample_indices)] . closed regions subsampling . Now we load in previous results regarding presence of motifs in windows, stored in motif_memberships.csv . motif_memberships=pd.read_csv(&#39;motif_memberships.csv&#39;) unique_motifs=set(motif_memberships[&#39;motif&#39;]) ... motif_memberships[&#39;motif_index&#39;] = motif_memberships[&#39;motif&#39;].map(motifs_dict) . Note that motif_memberships is a very large table with 3045603 rows. . &gt;&gt;&gt; motif_memberships chrom start stop motif float sign chrom_dict d_motif_membership 0 chr1 2203656 2203666 M00140 326.956217 + 13 918 1 chr1 4815881 4815891 M00140 111.470794 + 13 3329 2 chr1 5034577 5034587 M00140 8.410601 - 13 3579 3 chr1 5238635 5238645 M00140 4.346002 - 13 3783 4 chr1 5600926 5600936 M00140 4.346002 + 13 4146 ... ... ... ... ... ... ... ... ... 3045598 chrX 154944162 154944172 M00141 5.723951 - 4 1850169 3045599 chrY 19791318 19791328 M00141 9.169588 - 36 1866533 3045600 chrY 19791342 19791352 M00141 9.169588 - 36 1866533 3045601 chrY 23790632 23790642 M00141 5.723951 - 36 1866946 3045602 chrY 28430492 28430502 M00141 9.169588 + 36 1867233 [3045603 rows x 8 columns] . When we generate the T matrix, we will loop through rows of motif_memberships, and if the motif is present in a window, we will set the corresponding T matrix entry to 1. . So to save computation time, we want to avoid looping through rows which do not indicate the presence of a motif. . d_motif_membership stores the index of the window (indexed to windows) where a motif is present. It will be empty otherwise . all_present_windows is a vector containing the indices of all windows, starting from 1. . So, to remove the rows where a motif does not exist inside any applicable window, we just have to filter the motif_memberships table and remove all rows where d_motif_membership is empty, or does not exist in all_present_windows . &gt;&gt;&gt; motifs.head() chrom start stop motif float sign chrom_dict d_motif_membership 0 chr1 2203656 2203666 M00140 326.956217 + 13 918 1 chr1 4815881 4815891 M00140 111.470794 + 13 3329 2 chr1 5034577 5034587 M00140 8.410601 - 13 3579 3 chr1 5238635 5238645 M00140 4.346002 - 13 3783 4 chr1 5600926 5600936 M00140 4.346002 + 13 4146 . all_present_windows=chromatin_samples.index applicable_motifs=motif_memberships[motif_memberships.d_motif_membership.isin(all_present_windows)] . Now set up input arrays for GPU function . #1. #apply function to convert motif name to int motif_int = lambda x: int(x[1:]) start_time=time.time() #map motif name to integer to be used as column index for T matrix #eg &#39;M00001&#39; corresponds to column 1, &#39;M00002&#39; to column 2, etc a_motifs_list=list(applicable_motifs.motif) applicable_motifs_int= list(map(motif_int, a_motifs_list)) motif_name_instances = np.array(applicable_motifs_int,dtype=np.int32) . We only loop through windows where we know motifs are present (to save time) . #2. #motif_membership_windows is all window indices motif_membership_windows=np.array(applicable_motifs.d_motif_membership,dtype=np.int32) . Store all window indices in an array so that we can loop through them . #3. in_t_window_index #in_t_window_index is relevant window index (row of T matrix) t_window_index= np.array(chromatin_samples.index,dtype=np.int32) . Initialise the empty T matrix. . Note that for speed of computation, data type of Boolean has been used (0,1) only permissible values . These will be converted to 1 and 0 later during NN stage . #4. #out_t_matrix is empty array with: # columns corresponding to motif names (1372 of them) # rows corresponding to instance of training data (equivalent to nrow in t matrix) # boolean cos true/false in_t_matrix = np.empty(shape=(chromatin_samples.shape[0],1372),dtype=bool) in_t_matrix.fill(False) . Function definition. See earlier documents for illustration of how to initialise function on GPU array. . @cuda.jit def set_motifs(motif_name_instances, motif_membership_windows, t_window_index, t_matrix): #we are looping through two variables (motif number, window number), so we initialise a 2x2 grid on the GPU array T_row, M_row = cuda.grid(2) #motif row d1, d2 = cuda.gridsize(2) #t_rows is the number of windows we are checking t_rows=t_matrix.shape[0] #number of rows in the motif lookup table motif_rows=len(motif_name_instances) for _motif in range(M_row, motif_rows, d2): #each motif corresponds to relevant column of T matrix relevant_motif = motif_name_instances[_motif] #each window corresponds to row of T matrix relevant_window=motif_membership_windows[_motif] #now loop thru each window, and then each motif to check presence of motif in window for _row in range(T_row, t_rows, d1): if t_window_index[_row]==relevant_window: # set: # row = window index # column = motif index / name # motif exists here, so set to True t_matrix[_row,relevant_motif-1]=True . Assign partitions of training, validation and test data (more code related to sampling indices is in source code) . TRAINING_PERCENTAGE=0.75 VALIDATION_PERCENTAGE=0.125 TEST_PERCENTAGE=1-TRAINING_PERCENTAGE-VALIDATION_PERCENTAGE . Now we save out as feather data format which is lightweight (way better than CSV) . #write out as &#39;T_MATRIX.csv&#39; post_t_df.to_feather(&#39;T_MATRIX.feather&#39;,index=False) . Back to contents .",
    "url": "http://localhost:4000/docs/circuitSNP/code/5_generate_T_matrix/",
    "relUrl": "/docs/circuitSNP/code/5_generate_T_matrix/"
  }
  ,"6": {
    "title": "",
    "content": "Back to contents . In the previous step, we generated T matrix as input for NN . In this step, we will use the T matrix as input data . And the goal is to split the data into three types useful for the neural net: train, validation and test data . Source Code: ./split_data_for_Denny_NN.py . Run python script in bash terminal: . python ./split_data_for_Denny_NN.py . First load data: . t_matrix=pd.read_feather(&#39;T_matrix.feather&#39;) . Generate alternate column so that we can set target vector as 1-hot vector . def alternate(val): if int(val)==1: return 0 else: return 1 #append extra row for output #so that target will be 1-hot vector eg: # [1] = [1,0] # [0] = [0,1] T_matrix[&#39;Y_alt&#39;]=T_matrix[&#39;Y&#39;] T_matrix[&#39;Y_alt&#39;] = T_matrix[&#39;Y_alt&#39;].apply(alternate) . Now split into three different data types (validation, train, test) and save out . train=T_matrix.loc[T_matrix[&#39;data_type&#39;]==&#39;TRAINING_DATA&#39;].reset_index().drop(&#39;data_type&#39;,axis=1) test=T_matrix.loc[T_matrix[&#39;data_type&#39;]==&#39;TEST_DATA&#39;].reset_index().drop(&#39;data_type&#39;,axis=1) validation=T_matrix.loc[T_matrix[&#39;data_type&#39;]==&#39;VALIDATION_DATA&#39;].reset_index().drop(&#39;data_type&#39;,axis=1) validation.to_feather(&#39;validation_data.feather&#39;) test.to_feather(&#39;test.feather&#39;) train.to_feather(&#39;train.feather&#39;) . Back to contents .",
    "url": "http://localhost:4000/docs/circuitSNP/code/6_split_data_for_denny_NN/",
    "relUrl": "/docs/circuitSNP/code/6_split_data_for_denny_NN/"
  }
  ,"7": {
    "title": "6. Run NN",
    "content": "Back to contents . In the previous step, we split the data into train, validation and test . In this step, we will use the training and test data . And the goal is to build NN to predict whether chromatin open or closed based on which motifs are present in which window . Most of the code in this script was adapted from Denny’s work: . https://github.com/dennyshin/project_circuitSNP . Source Code: ./run_DShin_NN.py . Run python script in bash terminal . python ./run_DShin_NN.py . First load data and subset into X,Y partitions, then convert to numpy array . #use this as lookup function for all present motifs motif_names=[&#39;M&#39; + str(index).zfill(5) for index in range(1,1373)] #read in different data types validation=pd.read_feather(&#39;validation_data.feather&#39;) test=pd.read_feather(&#39;test.feather&#39;) train=pd.read_feather(&#39;train.feather&#39;) #training data Xtrain=train[motif_names] Xtrain=Xtrain.apply(pd.to_numeric).to_numpy() Ytrain=train[[&#39;Y&#39;,&#39;Y_alt&#39;]].to_numpy() #validation data Xval=validation[motif_names] Xval=Xval.apply(pd.to_numeric).to_numpy() Yval=validation[[&#39;Y&#39;,&#39;Y_alt&#39;]].to_numpy() . Further conversion of data for torch processing . #as we are using cuda to compute NN, set dtype as follows. This ensures it will run on GPU. dtype = torch.cuda.FloatTensor # convert data into torch tensors (so we can use pytorch to run) Xtrain = torch.from_numpy(Xtrain).type(dtype) Ytrain = torch.from_numpy(Ytrain).type(dtype) Xval = torch.from_numpy(Xval).type(dtype) Yval = torch.from_numpy(Yval).type(dtype) . Check proportions of open / closed chromatin are same for all data types . #check proportions for d in [train,validation,test]: print(str(d.Y.value_counts()[0]/d.Y.value_counts()[1])) # all at ~11 . 11.254522114599137 11.251305799364602 11.301038062283737 . As expected, there is a good mixing of open / closed chromatin regions in all data types . Force torch to use GPU . cuda = torch.device(&#39;cuda&#39;) . Instantiate NN in Torch . # build the neural net (D.Shin architecture) class Net(nn.Module): def __init__(self, input_dim): super().__init__() self.fc1 = nn.Linear(input_dim, 5) self.fc2 = nn.Linear(5, 5) self.fc3 = nn.Linear(5, 2) # these are still random for each new model we create nn.init.xavier_uniform_(self.fc1.weight) nn.init.xavier_uniform_(self.fc2.weight) nn.init.xavier_uniform_(self.fc3.weight) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) # do not put relu on the last layer! return F.softmax(self.fc3(x), dim=1) . Below parameters copied from Denny . # set model parameters (D.Shin) # define my model motif_num = Xtrain.shape[1] # this is now an int model = Net(motif_num).to(cuda) # choose optimizer learning_rate = 0.0001 optimizer = optim.Adam(model.parameters(), lr=learning_rate) # choose my criteria criterion = nn.BCELoss() . Send data to GPU . # send training data Xtrain, Ytrain = Xtrain.to(cuda), Ytrain.to(cuda) # send validation data Xval, Yval = Xval.to(cuda), Yval.to(cuda) . Now train . # train model (D.Shin) # training and validation n_epochs = 5000 epochs = list(range(1, n_epochs+1)) train_loss = [] val_loss = [] min_val_loss = 1 for epoch in epochs: model.train() # put the model in train mode optimizer.zero_grad() # null my gradients otherwise they will accumulate Ytrain_pred = model(Xtrain) # calculate my Y_hat loss = criterion(Ytrain_pred, Ytrain) # calculate my loss train_loss.append(loss) loss.backward(loss) # finds grad * loss (remember this is a weighted sum, where weight = loss) optimizer.step() # update my parameters model.eval() with torch.no_grad(): Yval_pred = model(Xval) loss = criterion(Yval_pred, Yval) val_loss.append(loss) if epoch % 100 ==0 or epoch==1: print(&quot;epoch: &quot;, epoch, f&quot;, val_loss: {loss.item(): f}&quot;) # save model with lowest validation loss if loss &lt; min_val_loss: min_val_loss = loss torch.save(model, &quot;trained_models/model.pt&quot;) opt_epoch = epoch . Results are validation loss ~0.28 (expected) . epoch: 1 , val_loss: 0.734370 epoch: 100 , val_loss: 0.722462 epoch: 200 , val_loss: 0.710027 epoch: 300 , val_loss: 0.697230 epoch: 400 , val_loss: 0.684107 epoch: 500 , val_loss: 0.670686 epoch: 600 , val_loss: 0.656984 epoch: 700 , val_loss: 0.641809 epoch: 800 , val_loss: 0.626065 epoch: 900 , val_loss: 0.610111 epoch: 1000 , val_loss: 0.594016 epoch: 1100 , val_loss: 0.577875 epoch: 1200 , val_loss: 0.561785 epoch: 1300 , val_loss: 0.545837 epoch: 1400 , val_loss: 0.530124 epoch: 1500 , val_loss: 0.513478 epoch: 1600 , val_loss: 0.495988 epoch: 1700 , val_loss: 0.479464 epoch: 1800 , val_loss: 0.463758 epoch: 1900 , val_loss: 0.448854 epoch: 2000 , val_loss: 0.434746 epoch: 2100 , val_loss: 0.421437 epoch: 2200 , val_loss: 0.408921 epoch: 2300 , val_loss: 0.397194 epoch: 2400 , val_loss: 0.386238 epoch: 2500 , val_loss: 0.376032 epoch: 2600 , val_loss: 0.366554 epoch: 2700 , val_loss: 0.357778 epoch: 2800 , val_loss: 0.349675 epoch: 2900 , val_loss: 0.342214 epoch: 3000 , val_loss: 0.335366 epoch: 3100 , val_loss: 0.329100 epoch: 3200 , val_loss: 0.323383 epoch: 3300 , val_loss: 0.318180 epoch: 3400 , val_loss: 0.313462 epoch: 3500 , val_loss: 0.309198 epoch: 3600 , val_loss: 0.305361 epoch: 3700 , val_loss: 0.301920 epoch: 3800 , val_loss: 0.298849 epoch: 3900 , val_loss: 0.296123 epoch: 4000 , val_loss: 0.293713 epoch: 4100 , val_loss: 0.291595 epoch: 4200 , val_loss: 0.289744 epoch: 4300 , val_loss: 0.288132 epoch: 4400 , val_loss: 0.286742 epoch: 4500 , val_loss: 0.285551 epoch: 4600 , val_loss: 0.284538 epoch: 4700 , val_loss: 0.283684 epoch: 4800 , val_loss: 0.282968 epoch: 4900 , val_loss: 0.282376 epoch: 5000 , val_loss: 0.281892 . So in summary after ~5000 epochs, validation loss at around 0.282 . Back to contents .",
    "url": "http://localhost:4000/docs/circuitSNP/code/7_run_NN/",
    "relUrl": "/docs/circuitSNP/code/7_run_NN/"
  }
  ,"8": {
    "title": "circuitSNP",
    "content": "circuit SNP . circuitSNP (Shanku et al., 2018) predicts the open or closed state of chromatin using the locations of SNPs… .",
    "url": "http://localhost:4000/docs/circuitSNP/circuitSNP/",
    "relUrl": "/docs/circuitSNP/circuitSNP/"
  }
  ,"9": {
    "title": "Python Implementation",
    "content": "This section contains instructions for data processing and analysis. Following steps are sequentially performed, and are detailed in the table of contents below. . Data acquisition | Data processing - analysing motifs present and chromatin status within windows | Building neural network | Validation of NN using dsqtl file | .",
    "url": "http://localhost:4000/docs/circuitSNP/code/code_documentation/",
    "relUrl": "/docs/circuitSNP/code/code_documentation/"
  }
  ,"10": {
    "title": "Imports and functions",
    "content": "All imports are here: . import pandas as pd import numpy as np import io import pickle import timeit import time import sys from numba import cuda, jit import copy from random import sample import os . Save path must be edited for compatibility on diff systems . #get save_path save_path=open(&#39;save_path.txt&#39;,&#39;r&#39;).read().splitlines()[0] #generate compiled and downloaded data directories COMP_DIR=save_path+&#39;compiled_data/&#39; DOWN_DIR=save_path+&#39;downloaded_data/&#39; MODEL_DIR=save_path+&#39;models/&#39; . Following code is deprecated, but keep for now . NUMBER_OF_MOTIFS=1372 motifs=[&#39;M&#39; + str(motif_string).zfill(5) for motif_string in range(1,NUMBER_OF_MOTIFS+1)] . Chromosome dict for mapping . #construct dictionary so that we can group by chromosome total_chrom=[&#39;chr&#39;+str(k+1) for k in range(22)]+[&#39;chrX&#39;,&#39;chrY&#39;] chromosome_dict={} for i,chrom in enumerate(total_chrom): chromosome_dict[chrom]=i #create dictionary to map chromosome to integer chr_dict_for_dsqtl={} for chrom,chr_int in zip(chromosome_dict.values(),chromosome_dict.keys()): chr_dict_for_dsqtl[chrom]=chr_int . Following motif code is deprecated, keep for now . #construct dictionary to convert motif to number #all_motif=[&#39;M&#39; + str(mot_num).zfill(5) for mot_num in range(1,NUMBER_OF_MOTIFS+1)] #motif_dict={} #for k,motif in enumerate(all_motif): # motif_dict[motif]=k+1 .",
    "url": "http://localhost:4000/docs/circuitSNP/code/functions/",
    "relUrl": "/docs/circuitSNP/code/functions/"
  }
  ,"11": {
    "title": "Home",
    "content": "PhD Repository . Please see the navigation pane on the left hand side for details of what I’m currently working on .",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  }
  ,"12": {
    "title": "Web Development",
    "content": "Web Dev notes for Jekyll / just-the-docs . Format Gemfile (only required for initialisation) . If no gemfile in root folder of repository directory, make one (no file extension) . touch Gemfile . Add following contents by using nano . sudo nano Gemfile . source &quot;https://rubygems.org&quot; gemspec gem &quot;jekyll&quot; . Initialise Jekyll Environment . After chroot to repository folder, enter following in terminal: . gem install jekyll bundler . Then launch site using following - this allows to make changes on the fly (markdown rendered to html with every save of .md file) . bundle exec jekyll serve . Then visit wherever home directory has been config (from separate terminal) . firefox 127.0.0.1:4000 .",
    "url": "http://localhost:4000/docs/jekyll_maintenance/jekyll_setup/",
    "relUrl": "/docs/jekyll_maintenance/jekyll_setup/"
  }
  ,"13": {
    "title": "Slurm example script",
    "content": "Example Slurm script: . Save this w file extension ‘.slurm’ . #job specifications (time, cpus, etc) #!/bin/bash #SBATCH --nodes 1 #SBATCH --account=punim0614 #SBATCH --qos=gpgpumdhs #SBATCH --partition gpgpu #SBATCH --gres=gpu:p100:1 #SBATCH --time 01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --job-name=circuitSNP_4 #load modules module load Python/3.6.10-intel-2017.u2-GCC-6.2.0-CUDA10.1 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA10.1-Python-3.6.10-GPU source ./p6/bin/activate #execute relevant python script python m4_d.py . Run using: . [apmoore@spartan-login1 circuitSNP]$ sbatch tf.slurm Submitted batch job 16346235 . job stdout will be written to slurm-16346235.out .",
    "url": "http://localhost:4000/docs/hpc/slurm/",
    "relUrl": "/docs/hpc/slurm/"
  }
  ,"14": {
    "title": "Spartan HPC Usage",
    "content": "Some notes on Spartan / HPC usage . Logging in . Use ssh &lt;username&gt;@spartan.hpc.unimelb.edu.au to login via ssh terminal in linux/mac or else use PUTTY on windows machine . Starting session or running whatever . When using spartan make sure to run code from a session - the default terminal interface runs on the login node and this is a common landing area for new users. . sinteractive session very useful for debugging. Once verified to run on this node, can batch to run on hpc cluster via slurm . Start up the unit with following, for eg 20 mins. Change acct as necessary. . sinteractive --nodes 1 --account=punim0614 --partition gpgpu --qos=gpgpumdhs --gres=gpu:p100:1 --time 01:00:00 --cpus-per-task=1 . For 1hr . sinteractive --nodes 1 --account=punim0614 --partition gpgpu --qos=gpgpumdhs --gres=gpu:p100:1 --time 01:00:00 --cpus-per-task=1 . To specify more memory (40000MB in this case). If you don’t have enough memory sometimes you will get ‘process killed’ error. . sinteractive --nodes 1 --account=punim0614 --partition gpgpu --qos=gpgpumdhs --mem 40000 --gres=gpu:p100:1 --time 01:00:00 --cpus-per-task=1 . Loading python modules . This is order of operation to set up environment for example relating to analysis for circuitSNP. The last command called in bash will overwrite any applicable libraries imported by other commands. For example, if importing python3.6.4 kernel, will backdate pandas to earlier version than that used in python3.6.10 kernel, but libraries not common to both will be left unaltered. This is particularly relevant for protobuf library which differs between various tensorflow versions, and the verison of pandas - as .pickle objects (serialised using pickle library in python) will not be compatible if the version of pandas used to save object is different from the verison used to load. . load python module (you would only load one python module) #for python with CUDA9 module load Python/3.6.4-intel-2017.u2-GCC-6.2.0-CUDA9 #for python with CUDA10 module load Python/3.6.10-intel-2017.u2-GCC-6.2.0-CUDA10.1 . | then load tensorflow #for cuda9 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA9-Python-3.6.4 #for cuda10 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA10.1-Python-3.6.10-GPU . | then load virutalenv cd circuitSNP source ./p6/bin/activate #in circuitSNP directory... . | All togeth for cuda9 . module load Python/3.6.4-intel-2017.u2-GCC-6.2.0-CUDA9 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA9-Python-3.6.4 cd circuitSNP source ./p6/bin/activate #in circuitSNP directory... . And for cuda10 . module load Python/3.6.10-intel-2017.u2-GCC-6.2.0-CUDA10.1 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA10.1-Python-3.6.10-GPU source ~/circuitSNP/p6c10/bin/activate #in circuitSNP directory... . Virtual environment . To create new virtual env do following (note change python version if need). Calling venv will attempt to load that python module. . It’s important to call module load web_proxy so that the server can surf the web and download more modules . module load Python/3.6.4-intel-2017.u2-GCC-6.2.0-CUDA9 module load web_proxy virtualenv &lt;venv name&gt;F . To install libraries for each cuda version: . Cuda9 . module load Python/3.6.4-intel-2017.u2-GCC-6.2.0-CUDA9 module load web_proxy virtualenv source venv/bin/activate module load web_proxy pip install pandas==0.22.0 pip install protobuf==3.8.0 . Cuda10 . module load Python/3.6.10-intel-2017.u2-GCC-6.2.0-CUDA10.1 module load web_proxy virtualenv p6c10 source p6c10/bin/activate pip install absl-py pip install scipy==1.4.1 pip install pandas==0.22.0 pip install matplotlib . To double check compatibility of various python modules, call following: . pip freeze . GPU usage w tensorflow . If using cuda10, we need to define model on the gpu device, otherwise tflow might run on CPU. . Force GPU usage using flag with tf.device(&#39;/device:GPU:0&#39;): then that instance of tf refers to the GPU version of tflow kernel . from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) #returns device as &#39;/device:GPU:0&#39; NUMBER_OF_EPOCHS=500 with tf.device(&#39;/device:GPU:0&#39;): Xtrain_v=tf.convert_to_tensor(Xtrain_v) Ytrain_v=tf.convert_to_tensor(Ytrain_v) Xval_v=tf.convert_to_tensor(Xval_v) Yval_v=tf.convert_to_tensor(Yval_v) Xtest_v=tf.convert_to_tensor(Xtest_v) Ytest_v=tf.convert_to_tensor(Ytest_v) model1 = tf.keras.Sequential() #model defined on GPU model1.add(tf.keras.layers.Flatten(input_shape=(1, 1372))) model1.add(tf.keras.layers.BatchNormalization()) model1.add(tf.keras.layers.Dense(5, activation=&#39;relu&#39;)) model1.add(tf.keras.layers.BatchNormalization()) model1.add(tf.keras.layers.Dense(units=3, activation=&#39;relu&#39;)) model1.add(tf.keras.layers.BatchNormalization()) model1.add(tf.keras.layers.Dense(units=1, activation=&#39;sigmoid&#39;)) model1.compile(optimizer=&#39;adagrad&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;]) history1 = model1.fit(x=Xtrain_v,y=Ytrain_v, epochs=NUMBER_OF_EPOCHS, batch_size=128,validation_data=(Xval_v,Yval_v)) model1.save(&#39;./models/model_batch_norm_every_5_3_128_.h5&#39;) . #####Misc / copying files . Copying files have following commands: . scp &lt;from_dir&gt; &lt;to_dir&gt; . Example on relevant data: . scp /home/doctorjonescore/Desktop/cdata/3_test_data.pickle apmoore@spartan.hpc.unimelb.edu.au:./circuitSNP/data/3_test_data.pickle scp /home/doctorjonescore/Desktop/cdata/3_train_data.pickle apmoore@spartan.hpc.unimelb.edu.au:./circuitSNP/data/3_train_data.pickle scp /home/doctorjonescore/Desktop/cdata/3_validation_data.pickle apmoore@spartan.hpc.unimelb.edu.au:./circuitSNP/data/3_validation_data.pickle . #####Slurm . Batch files using slurm, here example . #!/bin/bash #SBATCH --nodes 1 #SBATCH --account=punim0614 #SBATCH --qos=gpgpumdhs #SBATCH --partition gpgpu #SBATCH --gres=gpu:p100:1 #SBATCH --time 00:05:00 #SBATCH --cpus-per-task=1 module load Tensorflow/1.8.0-intel-2017.u2-GCC-6.2.0-CUDA9-Python-3.5.2-GPU python tensor_flow.py - .",
    "url": "http://localhost:4000/docs/hpc/spartan/",
    "relUrl": "/docs/hpc/spartan/"
  }
  ,"15": {
    "title": "",
    "content": "#####generate_SNP_vectors_6.py . In this stage, we wish to identify all SNPs in a region. We want to generate 3 outputs: . The aim: . Generate F vectors (both ref and alternate contained) | Generate reference vectors | Generate alternate vectors | Dependencies, functions, imports . from functions import * . Data downloaded in earlier steps: . SNP_READS is all SNPs . dsqtl consists of SNPs which are also dsQTLs . #read in SNPs SNP_READS=pd.read_csv(DOWN_DIR + &#39;compendium/all_compendium_reads.csv&#39;) SNP_READS[&#39;SNP_window_index&#39;]=-1 #read in dsQTLs dsqtl=pd.read_csv(DOWN_DIR + &#39;test/dsQTL.eval.txt&#39;,sep=&#39; t&#39;) dsqtl[&#39;SNP_window_index&#39;]=-1 . rename for easier referencing . SNP_READS.rename(columns = {&#39;pos (0-based)&#39;:&#39;snp_start&#39;}, inplace = True) SNP_READS.rename(columns = {&#39;pos1 (1-based)&#39;:&#39;snp_end&#39;}, inplace = True) SNP_READS.rename(columns = {&#39;motif (motif ID, see factorNames.txt)&#39;:&#39;motif&#39;}, inplace = True) dsqtl.rename(columns = {&#39;pos0&#39;:&#39;snp_start&#39;}, inplace = True) dsqtl.rename(columns = {&#39;pos&#39;:&#39;snp_end&#39;}, inplace = True) . before manipulating tables, number of rows in each is . &gt;&gt;&gt; SNP_READS.shape (5202041, 12) &gt;&gt;&gt; dsqtl.shape (27702, 24) . remove sex chromosomes . SNP_READS=SNP_READS[~SNP_READS.chr.isin([&#39;chrX&#39;,&#39;chrY&#39;])] dsqtl=dsqtl[~dsqtl.chr.isin([&#39;chrX&#39;,&#39;chrY&#39;])] . new number of rows in each . &gt;&gt;&gt; SNP_READS.shape (5057359, 12) &gt;&gt;&gt; dsqtl.shape (27702, 24) . Number of unique SNP is 3766427 . &gt;&gt;&gt; SNP_READS[[&#39;chr&#39;,&#39;snp_start&#39;,&#39;snp_end&#39;]].drop_duplicates().shape (3766427, 3) . But we should expect this number to be larger when we consider that duplicate SNP (same location and chromosome) will overlap with different SNPs . SNP_READS[[&#39;chr&#39;,&#39;snp_start&#39;,&#39;snp_end&#39;,&#39;motif&#39;]].drop_duplicates().shape (5057359, 4) . So exactly same as the number of rows in entire table . Number of unique dsQTL in file is . &gt;&gt;&gt; dsqtl[[&#39;chr&#39;,&#39;snp_start&#39;,&#39;snp_end&#39;]].drop_duplicates().shape (27702, 3) . we want to find out how many dsQTL exist in SNP_READS (or how many SNP are dsQTL) . can use merge function . note that set indicator=True gives extra column ._merge that counts outcome for particular row - whether row is common to both data frames, or only one . common_dsqtl_snp = pd.merge(dsqtl,SNP_READS[[&#39;chr&#39;,&#39;snp_start&#39;,&#39;motif&#39;]], on=[&#39;chr&#39;,&#39;snp_start&#39;],how=&#39;left&#39;,indicator=True) #we are interested in the number of items that matched in both (indicates dsqtl in all SNP) common_dsqtl_snp._merge.value_counts() . left_only indicates values that only exist in dsqtl table so number of SNP exist in both is 8898 (shown below) . &gt;&gt;&gt; common_dsqtl_snp._merge.value_counts() left_only 21849 both 8898 right_only 0 Name: _merge, dtype: int64 . verify this with a right join should get 8898 again . common_dsqtl_snp_right = pd.merge(dsqtl,SNP_READS[[&#39;chr&#39;,&#39;snp_start&#39;,&#39;motif&#39;]], on=[&#39;chr&#39;,&#39;snp_start&#39;],how=&#39;right&#39;,indicator=True) #we are interested in the number of items that matched in both (indicates dsqtl in all SNP) common_dsqtl_snp_right._merge.value_counts() . perfect - therefore we have 8898 SNP which are also dsQTL in SNP_READS . ... common_dsqtl_snp_right._merge.value_counts() right_only 5048461 both 8898 left_only 0 . read in locations of motifs . relevant_motifs=pd.read_feather(COMP_DIR + &#39;2_motifs_in_lcl_windows.feather&#39;) &gt;&gt;&gt; relevant_motifs.head() index chrom start stop motif float sign chrom_dict d_motif_membership 0 21 chr1 2203656 2203666 M00140 326.956217 + 70 918 1 38 chr1 4815881 4815891 M00140 111.470794 + 70 3329 2 42 chr1 5034577 5034587 M00140 8.410601 - 70 3579 3 50 chr1 5238635 5238645 M00140 4.346002 - 70 3783 . convert chromosme to int for to use in CUDA func . #convert chromsome to number... all_chromosomes=list(set(SNP_READS.chr)) chr_dict={} for i,c in enumerate(all_chromosomes): chr_dict[c]=i #convert chromosome to int SNP_READS[&#39;chr_int&#39;]=-1 SNP_READS=SNP_READS.assign(chr_int = lambda dataframe: dataframe[&#39;chr&#39;].map(lambda chr: chr_dict[chr])) dsqtl[&#39;chr_int&#39;]=-1 dsqtl=dsqtl.assign(chr_int = lambda dataframe: dataframe[&#39;chr&#39;].map(lambda chr: chr_dict[chr])) . convert motif to int to use in CUDA func . #convert motif to int from functions import * SNP_READS[&#39;motif_int&#39;]=SNP_READS[&#39;motif&#39;].map(motif_dict) . subset to motif only under consideration . #we only want to select motif &lt;= M1372 SNP_READS[SNP_READS.motif_int&lt;=1372] SNP_READS=SNP_READS[SNP_READS.motif_int&lt;=1372] . double check no NAN vals . &gt;&gt;&gt; SNP_READS.motif_int.apply(isnan).value_counts() False 3134652 Name: motif_int, dtype: int64 . So all values in motif_int are not NAN, so all are from motifs in 1-1372 . Now see how many unique SNP. We can use snp_start as all starts correspond uniquely to each individual SNP . SNP_READS[[&#39;chr_int&#39;,&#39;snp_start&#39;]].drop_duplicates().shape (2388566, 2) . So we have 2388566 individual SNP . How many of those are dsQTL - 4165 . unique_SNP=SNP_READS[[&#39;chr_int&#39;,&#39;snp_start&#39;]].drop_duplicates() reduced_dsqtl_snp = pd.merge(dsqtl,unique_SNP, on=[&#39;chr_int&#39;,&#39;snp_start&#39;],how=&#39;left&#39;,indicator=True) reduced_dsqtl_snp._merge.value_counts() left_only 23537 both 4165 right_only 0 . re-merge to map unique_SNP back . SNP_READS=pd.merge(SNP_READS,unique_SNP, on=[&#39;chr_int&#39;,&#39;snp_start&#39;],how=&#39;left&#39;,indicator=True) . check that all match up . &gt;&gt;&gt; SNP_READS._merge.value_counts() both 3134652 right_only 0 left_only 0 Name: _merge, dtype: int64 . move data onto CUDA array . in_snp_index=np.array(SNP_READS.SNP_INDEX,dtype=np.int32) in_snp_effect=np.array(SNP_READS.effect,dtype=np.int32) in_snp_motif=np.array(SNP_READS.motif_int,dtype=np.int32) in_snp_priorlodds=np.array(SNP_READS.ref_priorlodds,dtype=np.float32) in_snp_altlodds=np.array(SNP_READS.alt_priorlodds,dtype=np.float32) in_all_motifs=np.array(range(1,1373),dtype=np.int32) . #T matrix constructed from number of unique snp, and number of motif T_mat=np.empty([unique_SNP.shape[0],NUMBER_OF_MOTIFS],dtype=np.bool) #now generate placeholders for output matrices on CUDA memory out_T_matrix_F = cuda.device_array_like(T_mat) out_T_matrix_Ref = cuda.device_array_like(T_mat) out_T_matrix_Alt = cuda.device_array_like(T_mat) #run function .",
    "url": "http://localhost:4000/docs/circuitSNP/code/snp_vec_creation/",
    "relUrl": "/docs/circuitSNP/code/snp_vec_creation/"
  }
  ,"16": {
    "title": "Now we split data",
    "content": "Now we split data . In prev step, we created entire T matrix, now in this python file we create validation/test/train data from this. T_matrix is class, and we have to define N_OPEN or N_CLOSED, and we need to define validation / train percentage. We can set RANDOM_SEED to make our random selection deterministic . First randomly select from open and closed windows. . self.sample_open(all_windows) self.sample_closed(all_windows) . Combine the open and closed selections . self.combine_T() . Split into train val and test data . self.split_data(train_pc,val_pc) . Assign train / test / val categories to data . self.assign_splits() . The usage of this is that we call the class, and we can call it using a random seed, and then we can make the dataset deterministic .",
    "url": "http://localhost:4000/docs/circuitSNP/code/split_data/",
    "relUrl": "/docs/circuitSNP/code/split_data/"
  }
  ,"17": {
    "title": "Github doc",
    "content": "Github doc . Setting up . Open up _config.yml and append following lines: . url: &lt;home url &gt; baseurl: /&lt;repository name&gt; . Cloning repo . Get https link by perusing repository in browser. Click on button that says ‘Clone or download’ and copy https link . Then chdir to working folder on local machine and clone . git clone &lt;github_https_link&gt; &gt; Cloning into `&lt;repo_name&gt;`... &gt; remote: Counting objects: 10, done. &gt; remote: Compressing objects: 100% (8/8), done. &gt; remove: Total 10 (delta 1), reused 10 (delta 1) &gt; Unpacking objects: 100% (10/10), done. . You should be able to force your local revision to the remote repo by using . git push -f &lt;remote&gt; &lt;branch&gt; . (e.g. git push -f origin master). Leaving off &lt; remote &gt; and &lt; branch &gt; will force push all local branches that have set –set-upstream. . Initialising repository . Initialise repository with . git init . Adding to old repository . In Terminal, add the URL for the remote repository where your local repostory will be pushed. . git remote add origin .",
    "url": "http://localhost:4000/docs/github%20misc/github/",
    "relUrl": "/docs/github%20misc/github/"
  }
  ,"18": {
    "title": "",
    "content": "Copying data . In order to copy data, use following syntax: . scp ./3_train_data.feather apmoore@spartan.hpc.unimelb.edu.au:~/circuitSNP/data/3_train_data.feather . first argument: source file second argument: destination file (ssh via personal acct on spartan hpc cluster) . Misc Other HPCs . An SSH config file will also make your life easier. It allows you to create alises (i.e. shortcuts) for a given hostname. . Create the text file in your ~/.ssh directory with your preferred text editor, for example, nano. . nano .ssh/config . Enter the following (replacing username with your actual username of course!): . Host * ServerAliveInterval 120 Host spartan Hostname spartan.hpc.unimel . Load tflow - cpu . Module is called: . Tensorflow-CPU/1.15.0-spartan_intel-2017.u2-Python-3.6.4 .",
    "url": "http://localhost:4000/docs/hpc/copying_data_to_hpc/",
    "relUrl": "/docs/hpc/copying_data_to_hpc/"
  }
  
}