{
  
  "1": {
    "title": "Anaconda device",
    "content": "Anaconda . Two installed anaconda modules: . module load Anaconda2/5.3.0 module load Anaconda3/5.3.1 . To configure anaconda for bioconda . conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge . To import Cython - sometimes this is necessary to install packages (running setup wheel for setuptools.py etc) . module install Cython . Pybedtools . To import pybedtools: . module load Python/3.6.10-intel-2017.u2-GCC-6.2.0-CUDA10.1 module load pybedtools/0.7.10-intel-2017.u2-Python-3.6.4 source p6c10/bin/activate .",
    "url": "http://localhost:4000/docs/hpc/anaconda_notes/",
    "relUrl": "/docs/hpc/anaconda_notes/"
  }
  ,"2": {
    "title": "circuitSNP",
    "content": "circuit SNPs . Below are listed instructions for running circuitSNPs jupyter notebook .",
    "url": "http://localhost:4000/docs/circuitSNP/circuitSNP/",
    "relUrl": "/docs/circuitSNP/circuitSNP/"
  }
  ,"3": {
    "title": "Copying to HPC",
    "content": "Copying data . In order to copy data, use following syntax: . scp ./3_train_data.feather apmoore@spartan.hpc.unimelb.edu.au:~/circuitSNP/data/3_train_data.feather . first argument: source file second argument: destination file (ssh via personal acct on spartan hpc cluster) . Misc Other HPCs . An SSH config file will also make your life easier. It allows you to create alises (i.e. shortcuts) for a given hostname. . Create the text file in your /.ssh directory with your preferred text editor, for example, nano. . nano .ssh/config . Enter the following (replacing username with your actual username of course) . Load tflow - cpu . Module is called: . Tensorflow-CPU/1.15.0-spartan_intel-2017.u2-Python-3.6.4 .",
    "url": "http://localhost:4000/docs/hpc/copying_data_to_hpc/",
    "relUrl": "/docs/hpc/copying_data_to_hpc/"
  }
  ,"4": {
    "title": "Configuring site for GH hosting",
    "content": "Github doc . Setting up . Open up _config.yml and append following lines: . url: &lt;home url &gt; baseurl: /&lt;repository name&gt; . Cloning repo . Get https link by perusing repository in browser. Click on button that says ‘Clone or download’ and copy https link . Then chdir to working folder on local machine and clone . git clone &lt;github_https_link&gt; &gt; Cloning into `&lt;repo_name&gt;`... &gt; remote: Counting objects: 10, done. &gt; remote: Compressing objects: 100% (8/8), done. &gt; remove: Total 10 (delta 1), reused 10 (delta 1) &gt; Unpacking objects: 100% (10/10), done. . You should be able to force your local revision to the remote repo by using . git push -f &lt;remote&gt; &lt;branch&gt; . (e.g. git push -f origin master). Leaving off &lt; remote &gt; and &lt; branch &gt; will force push all local branches that have set –set-upstream. . Initialising repository . Initialise repository with . git init . Adding to old repository . In Terminal, add the URL for the remote repository where your local repostory will be pushed. . git remote add origin &lt;remote repository URL&gt; git remote add origin https://github.com/apmoore499/apmoore499.github.io.git . Github can’t host certain versionf of jekyll so use this code in Gemfile, also delete Gemfile.lock . gem &quot;jekyll&quot;, &quot;~&gt; 3.8.5&quot; gem &quot;github-pages&quot;,&quot;~&gt; 202&quot; , group: :jekyll_plugins gem &quot;just-the-docs&quot; group :jekyll_plugins do gem &quot;jekyll-feed&quot;, &quot;~&gt; 0.11.0&quot; end . If you need to add dependencies, you must do so in the package.json file, eg . &quot;dependencies&quot;: { &quot;minimist&quot;: &quot;&gt;=0.2.1&quot; } . original package.json: . { &quot;name&quot;: &quot;just-the-docs&quot;, &quot;version&quot;: &quot;0.2.8&quot;, &quot;description&quot;: &quot;A modern Jekyll theme for documentation&quot;, &quot;repository&quot;: &quot;pmarsceill/just-the-docs&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;bugs&quot;: &quot;https://github.com/pmarsceill/just-the-docs/issues&quot;, &quot;devDependencies&quot;: { &quot;@primer/css&quot;: &quot;^14.3.0&quot;, &quot;prettier&quot;: &quot;^2.0.5&quot;, &quot;stylelint&quot;: &quot;^13.3.3&quot;, &quot;stylelint-config-prettier&quot;: &quot;^8.0.1&quot;, &quot;stylelint-config-primer&quot;: &quot;^9.0.0&quot;, &quot;stylelint-prettier&quot;: &quot;^1.1.2&quot;, &quot;stylelint-selector-no-utility&quot;: &quot;^4.0.0&quot; }, &quot;scripts&quot;: { &quot;test&quot;: &quot;stylelint &#39;**/*.scss&#39;&quot;, &quot;format&quot;: &quot;prettier --write &#39;**/*.{scss,js,json}&#39;&quot;, &quot;stylelint-check&quot;: &quot;stylelint-config-prettier-check&quot; }, &quot;dependencies&quot;: {} } . updated package.json (note down the bottom dependencies list changed) . { &quot;name&quot;: &quot;just-the-docs&quot;, &quot;version&quot;: &quot;0.2.8&quot;, &quot;description&quot;: &quot;A modern Jekyll theme for documentation&quot;, &quot;repository&quot;: &quot;pmarsceill/just-the-docs&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;bugs&quot;: &quot;https://github.com/pmarsceill/just-the-docs/issues&quot;, &quot;devDependencies&quot;: { &quot;@primer/css&quot;: &quot;^14.3.0&quot;, &quot;prettier&quot;: &quot;^2.0.5&quot;, &quot;stylelint&quot;: &quot;^13.3.3&quot;, &quot;stylelint-config-prettier&quot;: &quot;^8.0.1&quot;, &quot;stylelint-config-primer&quot;: &quot;^9.0.0&quot;, &quot;stylelint-prettier&quot;: &quot;^1.1.2&quot;, &quot;stylelint-selector-no-utility&quot;: &quot;^4.0.0&quot; }, &quot;scripts&quot;: { &quot;test&quot;: &quot;stylelint &#39;**/*.scss&#39;&quot;, &quot;format&quot;: &quot;prettier --write &#39;**/*.{scss,js,json}&#39;&quot;, &quot;stylelint-check&quot;: &quot;stylelint-config-prettier-check&quot; }, &quot;dependencies&quot;: { &quot;minimist&quot;: &quot;&gt;=0.2.1&quot; } } .",
    "url": "http://localhost:4000/docs/jekyll_maintenance/github/",
    "relUrl": "/docs/jekyll_maintenance/github/"
  }
  ,"5": {
    "title": "Home",
    "content": "PhD Repository . Please see the navigation pane on the left hand side for details of what I’m currently working on .",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  }
  ,"6": {
    "title": "Web Development",
    "content": "Web Dev notes for Jekyll / just-the-docs . Format Gemfile (only required for initialisation) . If no gemfile in root folder of repository directory, make one (no file extension) . touch Gemfile . Add following contents by using nano . sudo nano Gemfile . source &quot;https://rubygems.org&quot; gemspec gem &quot;jekyll&quot; . Initialise Jekyll Environment . After chroot to repository folder, enter following in terminal: . gem install jekyll bundler . Then launch site using following - this allows to make changes on the fly (markdown rendered to html with every save of .md file) . bundle exec jekyll serve . Then visit wherever home directory has been config (from separate terminal) . firefox 127.0.0.1:4000 .",
    "url": "http://localhost:4000/docs/jekyll_maintenance/jekyll_setup/",
    "relUrl": "/docs/jekyll_maintenance/jekyll_setup/"
  }
  ,"7": {
    "title": "2. Starting Jupyter Notebook",
    "content": "Starting interactive session . Go to Spartan on demand https://spartan-ood.hpc.unimelb.edu.au/pun/sys/dashboard . . On Demand is a front-end to run jupyter notebook on the cluster . You will need to log in to your account. . Click on ‘my interactive sessions’ to go to this page: . . Then click on ‘jupyter notebook’ to go to this page: . . Then you need to fill out options in the fields like ‘Modules’, ‘Commands to run before loading the modules’ etc, keep going to the bottom of the page, . You link to your virtual environment (previous step) in the field ‘commands to run before jupyter’ - the virtual environment is sourced in the last command, ie ${HOME}/p27_jp/bin/activate . Once you have filled out options, click ‘launch’ button on bottom of page You will be taken to another page while you wait for session to start: . . . When session starts, you will be able to click on it to take you to the notebook. . This completes startup of notebook session. See options for fields below: . My field options: . Modules: . jupyter/1.0.0-GCC-6.2.0-Python-2.7.13 . Commands to run before loading the modules: . . Commands to run before Jupyter: . cd; module load Python/2.7.13-GCC-6.2.0; module load Tkinter/2.7.11-intel-2017.u2-Python-2.7.11; module load Python/2.7.13-GCC-6.2.0; module load Tensorflow/1.11.0-intel-2017.u2-Python-2.7.13-GPU; module load SAMtools/1.8-GCC-6.2.0-HTSlib-1.8; module load BEDTools/2.27.1-spartan_gcc-6.2.0; module load web_proxy; source ${HOME}/p27_jp/bin/activate; . Jupyter parameters: . . Account: . punim0614 . Partition: . gpgpu . QoS: . gpgpumdhs . Number of hours: . 1 . Number of CPU cores: . 1 . Number of GPUS: . 1 . Amount of RAM allocated to job: . 60000 . 60GB ram was adequate for my tasks, 20GB was sometimes too small. . Startup directory for Jupyter notebook: . /data/projects/punim0614/archer/csnp_mock/ . Misc notes: . For a different user, we may have to change QoS and account. The spartan sysadmin had to assign me a QoS so that I could access the gpgpu partition. . We start in /data/projects/punim0614/archer/csnp_mock/ because this is where all development code is stored. .",
    "url": "http://localhost:4000/docs/circuitSNP/jupyter_usage/",
    "relUrl": "/docs/circuitSNP/jupyter_usage/"
  }
  ,"8": {
    "title": "Slurm example script",
    "content": "Example Slurm script: . Save this w file extension ‘.slurm’ . #job specifications (time, cpus, etc) #!/bin/bash #SBATCH --nodes 1 #SBATCH --account=punim0614 #SBATCH --qos=gpgpumdhs #SBATCH --partition gpgpu #SBATCH --gres=gpu:p100:1 #SBATCH --time 01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --job-name=circuitSNP_4 #load modules module load Python/3.6.10-intel-2017.u2-GCC-6.2.0-CUDA10.1 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA10.1-Python-3.6.10-GPU source ./p6/bin/activate #execute relevant python script python m4_d.py . Run using: . [apmoore@spartan-login1 circuitSNP]$ sbatch tf.slurm Submitted batch job 16346235 . job stdout will be written to slurm-16346235.out .",
    "url": "http://localhost:4000/docs/hpc/slurm/",
    "relUrl": "/docs/hpc/slurm/"
  }
  ,"9": {
    "title": "Spartan HPC Usage",
    "content": "Some notes on Spartan / HPC usage . Logging in . Use ssh &lt;username&gt;@spartan.hpc.unimelb.edu.au to login via ssh terminal in linux/mac or else use PUTTY on windows machine . Starting session or running whatever . When using spartan make sure to run code from a session - the default terminal interface runs on the login node and this is a common landing area for new users. . sinteractive session very useful for debugging. Once verified to run on this node, can batch to run on hpc cluster via slurm . Start up the unit with following, for eg 20 mins. Change acct as necessary. . sinteractive --nodes 1 --account=punim0614 --partition gpgpu --qos=gpgpumdhs --gres=gpu:p100:1 --time 01:00:00 --cpus-per-task=1 . For 1hr . sinteractive --nodes 1 --account=punim0614 --partition gpgpu --qos=gpgpumdhs --gres=gpu:p100:1 --time 01:00:00 --cpus-per-task=1 . To specify more memory (40000MB in this case). If you don’t have enough memory sometimes you will get ‘process killed’ error. . sinteractive --nodes 1 --account=punim0614 --partition gpgpu --qos=gpgpumdhs --mem 40000 --gres=gpu:p100:1 --time 01:00:00 --cpus-per-task=1 . Loading python modules . This is order of operation to set up environment for example relating to analysis for circuitSNP. The last command called in bash will overwrite any applicable libraries imported by other commands. For example, if importing python3.6.4 kernel, will backdate pandas to earlier version than that used in python3.6.10 kernel, but libraries not common to both will be left unaltered. This is particularly relevant for protobuf library which differs between various tensorflow versions, and the verison of pandas - as .pickle objects (serialised using pickle library in python) will not be compatible if the version of pandas used to save object is different from the verison used to load. . load python module (you would only load one python module) #for python with CUDA9 module load Python/3.6.4-intel-2017.u2-GCC-6.2.0-CUDA9 #for python with CUDA10 module load Python/3.6.10-intel-2017.u2-GCC-6.2.0-CUDA10.1 . | then load tensorflow #for cuda9 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA9-Python-3.6.4 #for cuda10 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA10.1-Python-3.6.10-GPU . | then load virutalenv cd circuitSNP source ./p6/bin/activate #in circuitSNP directory... . | All togeth for cuda9 . module load Python/3.6.4-intel-2017.u2-GCC-6.2.0-CUDA9 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA9-Python-3.6.4 cd circuitSNP source ./p6/bin/activate #in circuitSNP directory... . And for cuda10 . module load Python/3.6.10-intel-2017.u2-GCC-6.2.0-CUDA10.1 module load Tensorflow/2.1.0-intel-2017.u2-GCC-6.2.0-CUDA10.1-Python-3.6.10-GPU source ~/circuitSNP/p6c10/bin/activate #in circuitSNP directory... . Virtual environment . To create new virtual env do following (note change python version if need). Calling venv will attempt to load that python module. . It’s important to call module load web_proxy so that the server can surf the web and download more modules . module load Python/3.6.4-intel-2017.u2-GCC-6.2.0-CUDA9 module load web_proxy virtualenv &lt;venv name&gt;F . To install libraries for each cuda version: . Cuda9 . module load Python/3.6.4-intel-2017.u2-GCC-6.2.0-CUDA9 module load web_proxy virtualenv source venv/bin/activate module load web_proxy pip install pandas==0.22.0 pip install protobuf==3.8.0 . Cuda10 . module load Python/3.6.10-intel-2017.u2-GCC-6.2.0-CUDA10.1 module load web_proxy virtualenv p6c10 source p6c10/bin/activate pip install absl-py pip install scipy==1.4.1 pip install pandas==0.22.0 pip install matplotlib . To double check compatibility of various python modules, call following: . pip freeze . GPU usage w tensorflow . If using cuda10, we need to define model on the gpu device, otherwise tflow might run on CPU. . Force GPU usage using flag with tf.device(&#39;/device:GPU:0&#39;): then that instance of tf refers to the GPU version of tflow kernel . from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) #returns device as &#39;/device:GPU:0&#39; NUMBER_OF_EPOCHS=500 with tf.device(&#39;/device:GPU:0&#39;): Xtrain_v=tf.convert_to_tensor(Xtrain_v) Ytrain_v=tf.convert_to_tensor(Ytrain_v) Xval_v=tf.convert_to_tensor(Xval_v) Yval_v=tf.convert_to_tensor(Yval_v) Xtest_v=tf.convert_to_tensor(Xtest_v) Ytest_v=tf.convert_to_tensor(Ytest_v) model1 = tf.keras.Sequential() #model defined on GPU model1.add(tf.keras.layers.Flatten(input_shape=(1, 1372))) model1.add(tf.keras.layers.BatchNormalization()) model1.add(tf.keras.layers.Dense(5, activation=&#39;relu&#39;)) model1.add(tf.keras.layers.BatchNormalization()) model1.add(tf.keras.layers.Dense(units=3, activation=&#39;relu&#39;)) model1.add(tf.keras.layers.BatchNormalization()) model1.add(tf.keras.layers.Dense(units=1, activation=&#39;sigmoid&#39;)) model1.compile(optimizer=&#39;adagrad&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;]) history1 = model1.fit(x=Xtrain_v,y=Ytrain_v, epochs=NUMBER_OF_EPOCHS, batch_size=128,validation_data=(Xval_v,Yval_v)) model1.save(&#39;./models/model_batch_norm_every_5_3_128_.h5&#39;) . #####Misc / copying files . Copying files have following commands: . scp &lt;from_dir&gt; &lt;to_dir&gt; . Example on relevant data: . scp /home/doctorjonescore/Desktop/cdata/3_test_data.pickle apmoore@spartan.hpc.unimelb.edu.au:./circuitSNP/data/3_test_data.pickle scp /home/doctorjonescore/Desktop/cdata/3_train_data.pickle apmoore@spartan.hpc.unimelb.edu.au:./circuitSNP/data/3_train_data.pickle scp /home/doctorjonescore/Desktop/cdata/3_validation_data.pickle apmoore@spartan.hpc.unimelb.edu.au:./circuitSNP/data/3_validation_data.pickle . #####Slurm . Batch files using slurm, here example . #!/bin/bash #SBATCH --nodes 1 #SBATCH --account=punim0614 #SBATCH --qos=gpgpumdhs #SBATCH --partition gpgpu #SBATCH --gres=gpu:p100:1 #SBATCH --time 00:05:00 #SBATCH --cpus-per-task=1 module load Tensorflow/1.8.0-intel-2017.u2-GCC-6.2.0-CUDA9-Python-3.5.2-GPU python tensor_flow.py - .",
    "url": "http://localhost:4000/docs/hpc/spartan/",
    "relUrl": "/docs/hpc/spartan/"
  }
  ,"10": {
    "title": "Step 1. Set up venv",
    "content": "Setting up virtual environment. . Instructions to set up python virtual environment on hpc cluster. . Once the virtual environment is set up, you should be able to run all code in jupyter notebook. . You will need to have access to gpgpu partition. . You need to set up a virtualenv to run the circuitSNPs jupyter notebook. Virtualenv keeps all the python libraries in one place - this is useful because libraries are always being updated, so if you want to run code that you worked on a year ago, often it won’t work with new libraries. . On spartan, we can set up virtualenv as follows: . Step 1. Log in to spartan account via terminal . | . . All following instructions involve typing commands into bash terminal. . Step 2. Run interactive session on gpgpu node - we need GPGPU . | . Options for different user may change. . I needed to set --qos=gpgpumdhs, please ask spartan helpdesk if having problems. . sinteractive --nodes 1 --account=punim0614 --partition gpgpu --qos=gpgpumdhs --mem 20000 --gres=gpu:p100:1 --time 00:30:00 --cpus-per-task=1 . Step 3. Change to home directory, in order import all necessary modules to run code . | . Copy paste this command to load all necessary modules for jupyter notebook . cd; module load Python/2.7.13-GCC-6.2.0; module load Tkinter/2.7.11-intel-2017.u2-Python-2.7.11; module load Python/2.7.13-GCC-6.2.0; module load Tensorflow/1.11.0-intel-2017.u2-Python-2.7.13-GPU; module load SAMtools/1.8-GCC-6.2.0-HTSlib-1.8; module load BEDTools/2.27.1-spartan_gcc-6.2.0; module load web_proxy . Step 4. Make virtualenv . | . Now we make virtual environment. . p27_jp is the name I have assigned for virtual environment. the name doesn’t matter - call it whatever you want. Just make sure you reference the name properly when you load Jupyter notebook (explained below). . Change to home directory (ie using command cd) to keep virtual environment here. I’m assuming that you can put it elsewhere, but this configuration worked for me. . Create env using command below: . virtualenv p27_jp . This will take maybe 1-5 minutes. . Step 5. Enable virtualenv . | . Now we enable virtual env as below . source p27_jp/bin/activate . Now you’re inside the virtual environment. Any libraries that you install here using pip will stay here. Any libraries you have previously installed while inside this environment will be accessible when you want to use it in future. . Step 6. Install numpy first . | . You’ll see a lot of red messages - don’t worry . pip install numpy==1.16.6 . At this stage, you need to read 3 separate lists of installation dependencies. I’ve stored them in my folder on the cluster - see path below. They are also stored within this github repository - if you download the repo, they are in main folder called ‘handover_installs’ . Again, you will see some red messages when installing - this is expected . Step 7. Install first list of dependencies . | . pip install -r /data/projects/punim0614/archer/handover_installs/pip_inst_1.txt . Step 8. Then install second list of dependencies . | . pip install -r /data/projects/punim0614/archer/handover_installs/pip_inst_2.txt . Step 9. Then install third list of dependencies . | . pip install -r /data/projects/punim0614/archer/handover_installs/pip_inst_3.txt . Step 10. Deactivate virtual environment . | . deactivate . Step 11. Deactivate sinteractive session . | . logout . Ok now we can run jupyter - virtual environment is set up! .",
    "url": "http://localhost:4000/docs/circuitSNP/venv/",
    "relUrl": "/docs/circuitSNP/venv/"
  }
  
}