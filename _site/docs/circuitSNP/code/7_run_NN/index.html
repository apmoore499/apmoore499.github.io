<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><title>6. Run NN - Archer</title><link rel="shortcut icon" href="http://localhost:4000/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="http://localhost:4000/assets/css/just-the-docs.css"> <script type="text/javascript" src="http://localhost:4000/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="http://localhost:4000/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Run NN | Archer</title><meta name="generator" content="Jekyll v3.8.5" /><meta property="og:title" content="Run NN" /><meta property="og:locale" content="en_US" /><meta name="description" content="PhD repository" /><meta property="og:description" content="PhD repository" /><link rel="canonical" href="http://localhost:4000/docs/circuitSNP/code/7_run_NN/" /><meta property="og:url" content="http://localhost:4000/docs/circuitSNP/code/7_run_NN/" /><meta property="og:site_name" content="Archer" /> <script type="application/ld+json"> {"@type":"WebPage","url":"http://localhost:4000/docs/circuitSNP/code/7_run_NN/","headline":"Run NN","description":"PhD repository","@context":"https://schema.org"}</script><body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="link" viewBox="0 0 16 16"><title>Link</title><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path> </symbol> </svg><div class="page-wrap"><div class="side-bar"><div class="site-header"> <a href="http://localhost:4000" class="site-title lh-tight"> Archer </a> <button class="menu-button fs-3 js-main-nav-trigger" data-text-toggle="Hide" type="button">Menu</button></div><div class="navigation main-nav js-main-nav"><nav role="navigation" aria-label="Main navigation"><ul class="navigation-list"><li class="navigation-list-item active"><a href="http://localhost:4000/docs/circuitSNP/code/6_split_data_for_denny_NN/" class="navigation-list-link"></a><li class="navigation-list-item active"><a href="http://localhost:4000/docs/circuitSNP/code/snp_vec_creation/" class="navigation-list-link"></a><li class="navigation-list-item"><a href="http://localhost:4000/docs/circuitSNP/code/split_data/" class="navigation-list-link">Now we split data</a><li class="navigation-list-item"><a href="http://localhost:4000/docs/github%20misc/github/" class="navigation-list-link">Github doc</a><li class="navigation-list-item active"><a href="http://localhost:4000/docs/hpc/copying_data_to_hpc/" class="navigation-list-link"></a><li class="navigation-list-item"><a href="http://localhost:4000/" class="navigation-list-link">Home</a><li class="navigation-list-item"><a href="http://localhost:4000/docs/circuitSNP/circuitSNP/" class="navigation-list-link">circuitSNP</a><ul class="navigation-list-child-list "><li class="navigation-list-item active"><a href="http://localhost:4000/docs/circuitSNP/code/code_documentation/" class="navigation-list-link">Python Implementation</a><ul class="navigation-list-child-list"><li class="navigation-list-item "> <a href="http://localhost:4000/docs/circuitSNP/code/1_preliminary_setup/" class="navigation-list-link">1. Environment Configuration</a><li class="navigation-list-item "> <a href="http://localhost:4000/docs/circuitSNP/code/2_download_data/" class="navigation-list-link">2. Download data</a><li class="navigation-list-item "> <a href="http://localhost:4000/docs/circuitSNP/code/3_analyse_lcl_chromatin/" class="navigation-list-link">3. Detect open / closed regions</a><li class="navigation-list-item "> <a href="http://localhost:4000/docs/circuitSNP/code/4_analyse_motif_window_membership/" class="navigation-list-link">4. Motif membership</a><li class="navigation-list-item "> <a href="http://localhost:4000/docs/circuitSNP/code/5_generate_T_matrix/" class="navigation-list-link">5. Generate T matrix</a><li class="navigation-list-item active"> <a href="http://localhost:4000/docs/circuitSNP/code/7_run_NN/" class="navigation-list-link active">6. Run NN</a><li class="navigation-list-item "> <a href="http://localhost:4000/docs/circuitSNP/code/functions/" class="navigation-list-link">Imports and functions</a></ul></ul><li class="navigation-list-item"><a href="http://localhost:4000/docs/hpc/spartan/" class="navigation-list-link">Spartan HPC Usage</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="http://localhost:4000/docs/hpc/slurm/" class="navigation-list-link">Slurm example script</a></ul><li class="navigation-list-item"><a href="http://localhost:4000/docs/jekyll_maintenance/jekyll_setup/" class="navigation-list-link">Web Development</a></ul></nav></div><footer class="site-footer"><p class="text-small text-grey-dk-000 mb-4">This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</p></footer></div><div class="main-content-wrap js-main-content" tabindex="0"><div class="main-content"><div class="page-header js-page-header"><div class="search"><div class="search-input-wrap"> <input type="text" class="js-search-input search-input" tabindex="0" placeholder="Search Archer" aria-label="Search Archer" autocomplete="off"> <svg width="14" height="14" viewBox="0 0 28 28" xmlns="http://www.w3.org/2000/svg" class="search-icon"><title>Search</title><g fill-rule="nonzero"><path d="M17.332 20.735c-5.537 0-10-4.6-10-10.247 0-5.646 4.463-10.247 10-10.247 5.536 0 10 4.601 10 10.247s-4.464 10.247-10 10.247zm0-4c3.3 0 6-2.783 6-6.247 0-3.463-2.7-6.247-6-6.247s-6 2.784-6 6.247c0 3.464 2.7 6.247 6 6.247z"/><path d="M11.672 13.791L.192 25.271 3.02 28.1 14.5 16.62z"/></g></svg></div><div class="js-search-results search-results-wrap"></div></div></div><div class="page"><nav class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/docs/hpc/copying_data_to_hpc/">Python Implementation</a><li class="breadcrumb-nav-list-item"><span>6. Run NN</span></ol></nav><div id="main-content" class="page-content" role="main"><h5 id="back-to-contents"> <a href="#back-to-contents" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> <a href="./..">Back to contents</a></h5><p>In the previous step, we split the data into train, validation and test</p><p>In this step, we will use the training and test data</p><p>And the goal is to build NN to predict whether chromatin open or closed based on which motifs are present in which window</p><p>Most of the code in this script was adapted from Dennyâ€™s work:</p><p><a href="https://github.com/dennyshin/project_circuitSNP">https://github.com/dennyshin/project_circuitSNP</a></p><h5 id="source-code-run_dshin_nnpy"> <a href="#source-code-run_dshin_nnpy" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Source Code: <a href="../run_DShin_NN.py">./run_DShin_NN.py</a></h5><p>Run python script in bash terminal</p><div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python ./run_DShin_NN.py
</code></pre></div></div><p>First load data and subset into X,Y partitions, then convert to numpy array</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#use this as lookup function for all present motifs
</span><span class="n">motif_names</span><span class="o">=</span><span class="p">[</span><span class="s">'M'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1373</span><span class="p">)]</span>

<span class="c1">#read in different data types
</span><span class="n">validation</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_feather</span><span class="p">(</span><span class="s">'validation_data.feather'</span><span class="p">)</span>
<span class="n">test</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_feather</span><span class="p">(</span><span class="s">'test.feather'</span><span class="p">)</span>
<span class="n">train</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_feather</span><span class="p">(</span><span class="s">'train.feather'</span><span class="p">)</span>

<span class="c1">#training data
</span><span class="n">Xtrain</span><span class="o">=</span><span class="n">train</span><span class="p">[</span><span class="n">motif_names</span><span class="p">]</span>
<span class="n">Xtrain</span><span class="o">=</span><span class="n">Xtrain</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">)</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">Ytrain</span><span class="o">=</span><span class="n">train</span><span class="p">[[</span><span class="s">'Y'</span><span class="p">,</span><span class="s">'Y_alt'</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1">#validation data
</span><span class="n">Xval</span><span class="o">=</span><span class="n">validation</span><span class="p">[</span><span class="n">motif_names</span><span class="p">]</span>
<span class="n">Xval</span><span class="o">=</span><span class="n">Xval</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">)</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">Yval</span><span class="o">=</span><span class="n">validation</span><span class="p">[[</span><span class="s">'Y'</span><span class="p">,</span><span class="s">'Y_alt'</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</code></pre></div></div><p>Further conversion of data for torch processing</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#as we are using cuda to compute NN, set dtype as follows. This ensures it will run on GPU.
</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span>
<span class="c1"># convert data into torch tensors (so we can use pytorch to run)
</span><span class="n">Xtrain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">Ytrain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Ytrain</span><span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">Xval</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Xval</span><span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">Yval</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Yval</span><span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

</code></pre></div></div><p>Check proportions of open / closed chromatin are same for all data types</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#check proportions
</span><span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="n">train</span><span class="p">,</span><span class="n">validation</span><span class="p">,</span><span class="n">test</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">d</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="mi">1</span><span class="p">]))</span>
<span class="c1"># all at ~11
</span></code></pre></div></div><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mf">11.254522114599137</span>
<span class="mf">11.251305799364602</span>
<span class="mf">11.301038062283737</span>
</code></pre></div></div><p>As expected, there is a good mixing of open / closed chromatin regions in all data types</p><p>Force torch to use GPU</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span> 
</code></pre></div></div><p>Instantiate NN in Torch</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># build the neural net (D.Shin architecture)
</span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
		<span class="c1"># these are still random for each new model we create
</span>		<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
		<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
		<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
		<span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
		<span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
		<span class="c1"># do not put relu on the last layer!
</span>		<span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


</code></pre></div></div><p>Below parameters copied from Denny</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># set model parameters (D.Shin)
# define my model
</span><span class="n">motif_num</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># this is now an int
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">motif_num</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda</span><span class="p">)</span>

<span class="c1"># choose optimizer
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># choose my criteria
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
</code></pre></div></div><p>Send data to GPU</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># send training data
</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ytrain</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda</span><span class="p">),</span> <span class="n">Ytrain</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda</span><span class="p">)</span>
<span class="c1"># send validation data
</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span> <span class="o">=</span> <span class="n">Xval</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda</span><span class="p">),</span> <span class="n">Yval</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda</span><span class="p">)</span>
</code></pre></div></div><p>Now train</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train model (D.Shin)
# training and validation
</span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">min_val_loss</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
	<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># put the model in train mode
</span>	<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># null my gradients otherwise they will accumulate
</span>	<span class="n">Ytrain_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)</span> <span class="c1"># calculate my Y_hat
</span>	<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">Ytrain_pred</span><span class="p">,</span> <span class="n">Ytrain</span><span class="p">)</span> <span class="c1"># calculate my loss
</span>	<span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
	<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="c1"># finds grad * loss (remember this is a weighted sum, where weight = loss)
</span>	<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># update my parameters
</span>	<span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
	<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
		<span class="n">Yval_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xval</span><span class="p">)</span>
	<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">Yval_pred</span><span class="p">,</span> <span class="n">Yval</span><span class="p">)</span>
	<span class="n">val_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
	<span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
		<span class="k">print</span><span class="p">(</span><span class="s">"epoch: "</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">f</span><span class="s">", val_loss: {loss.item(): f}"</span><span class="p">)</span>
	<span class="c1"># save model with lowest validation loss
</span>	<span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">min_val_loss</span><span class="p">:</span>
		<span class="n">min_val_loss</span> <span class="o">=</span> <span class="n">loss</span>
		<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">"trained_models/model.pt"</span><span class="p">)</span>
		<span class="n">opt_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
</code></pre></div></div><p>Results are validation loss ~0.28 (expected)</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epoch</span><span class="p">:</span>  <span class="mi">1</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.734370</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">100</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.722462</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">200</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.710027</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">300</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.697230</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">400</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.684107</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">500</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.670686</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">600</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.656984</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">700</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.641809</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">800</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.626065</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">900</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.610111</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1000</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.594016</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1100</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.577875</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1200</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.561785</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1300</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.545837</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1400</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.530124</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1500</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.513478</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1600</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.495988</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1700</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.479464</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1800</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.463758</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">1900</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.448854</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2000</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.434746</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2100</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.421437</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2200</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.408921</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2300</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.397194</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2400</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.386238</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2500</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.376032</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2600</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.366554</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2700</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.357778</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2800</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.349675</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">2900</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.342214</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3000</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.335366</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3100</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.329100</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3200</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.323383</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3300</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.318180</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3400</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.313462</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3500</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.309198</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3600</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.305361</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3700</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.301920</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3800</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.298849</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">3900</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.296123</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4000</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.293713</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4100</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.291595</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4200</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.289744</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4300</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.288132</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4400</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.286742</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4500</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.285551</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4600</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.284538</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4700</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.283684</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4800</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.282968</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">4900</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.282376</span>
<span class="n">epoch</span><span class="p">:</span>  <span class="mi">5000</span> <span class="p">,</span> <span class="n">val_loss</span><span class="p">:</span>  <span class="mf">0.281892</span>
</code></pre></div></div><p>So in summary after ~5000 epochs, validation loss at around 0.282</p><h5 id="back-to-contents-1"> <a href="#back-to-contents-1" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> <a href="./..">Back to contents</a></h5></div></div></div></div>
